{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过创建data.Dataset子类Mydataset来创建输入\n",
    "class Mydataset(data.Dataset):\n",
    "# 类初始化\n",
    "    def __init__(self, root):\n",
    "        self.imgs_path = root\n",
    "# 进行切片\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs_path[index]\n",
    "        return img_path\n",
    "# 返回长度\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(signature_dataset[\u001b[38;5;241m12\u001b[39m:\u001b[38;5;241m15\u001b[39m])\u001b[38;5;66;03m#切片，显示第12至第十五张图片的路径\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sinature_datalodaer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(signature_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m#每次迭代时返回五个数据\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msinature_datalodaer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m species \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m species_to_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((c, i) \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(species))\n",
      "File \u001b[0;32m~/anaconda3/envs/mix/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/mix/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/anaconda3/envs/mix/lib/python3.9/site-packages/torch/utils/data/dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用glob方法来获取数据图片的所有路径\n",
    "all_imgs_path = glob.glob(r'./home/lyc/doc/AML_LAB/imu_data/*.csv')#数据文件夹路径，根据实际情况更改！\n",
    "#循环遍历输出列表中的每个元素，显示出每个图片的路径\n",
    "for var in all_imgs_path:\n",
    "    print(var)\n",
    "\n",
    "#利用自定义类Mydataset创建对象weather_dataset\n",
    "signature_dataset = Mydataset(all_imgs_path)\n",
    "print(len(signature_dataset)) #返回文件夹中图片总个数\n",
    "print(signature_dataset[12:15])#切片，显示第12至第十五张图片的路径\n",
    "sinature_datalodaer = torch.utils.data.DataLoader(signature_dataset, batch_size=5) #每次迭代时返回五个数据\n",
    "print(next(iter(sinature_datalodaer)))\n",
    "\n",
    "species = ['False','True']\n",
    "species_to_id = dict((c, i) for i, c in enumerate(species))\n",
    "print(species_to_id)\n",
    "id_to_species = dict((v, k) for k, v in species_to_id.items())\n",
    "print(id_to_species)\n",
    "all_labels = []\n",
    "#对所有图片路径进行迭代\n",
    "for img in all_imgs_path:\n",
    "    # 区分出每个img，应该属于什么类别\n",
    "    for i, c in enumerate(species):\n",
    "        if c in img:\n",
    "            all_labels.append(i)\n",
    "print(all_labels) #得到所有标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行转换处理\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((240,320)), #做的第一步转换\n",
    "                transforms.ToTensor() #第二步转换，作用：第一转换成Tensor，第二将图片取值范围转换成0-1之间，第三会将channel置前\n",
    "])\n",
    "\n",
    "class Mydatasetpro(data.Dataset):\n",
    "# 类初始化\n",
    "    def __init__(self, img_paths, labels, transform):\n",
    "        self.imgs = img_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transform\n",
    "# 进行切片\n",
    "    def __getitem__(self, index):                #根据给出的索引进行切片，并对其进行数据处理转换成Tensor，返回成Tensor\n",
    "        img = self.imgs[index]\n",
    "        label = self.labels[index]\n",
    "        pil_img = Image.open(img)                 #pip install pillow\n",
    "        data = self.transforms(pil_img)\n",
    "        return data, label\n",
    "# 返回长度\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "signature_dataset = Mydatasetpro(all_imgs_path, all_labels, transform)\n",
    "sinature_datalodaer = data.DataLoader(\n",
    "                            signature_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True\n",
    ")\n",
    "\n",
    "imgs_batch, labels_batch = next(iter(sinature_datalodaer))\n",
    "print(imgs_batch.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (img, label) in enumerate(zip(imgs_batch[:6], labels_batch[:6])):\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.title(id_to_species.get(label.item()))\n",
    "    plt.imshow(img)\n",
    "plt.show()#展示图片\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#划分测试集和训练集\n",
    "index = np.random.permutation(len(all_imgs_path))\n",
    "\n",
    "all_imgs_path = np.array(all_imgs_path)[index]\n",
    "all_labels = np.array(all_labels)[index]\n",
    "\n",
    "#80% as train\n",
    "s = int(len(all_imgs_path)*0.7)\n",
    "print(s)\n",
    "\n",
    "train_imgs = all_imgs_path[:s]\n",
    "train_labels = all_labels[:s]\n",
    "test_imgs = all_imgs_path[s:]\n",
    "test_labels = all_labels[s:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(test_labels)\n",
    "train_ds = Mydatasetpro(train_imgs, train_labels, transform) #TrainSet TensorData\n",
    "test_ds = Mydatasetpro(test_imgs, test_labels, transform) #TestSet TensorData\n",
    "\n",
    "print(train_ds)\n",
    "train_imgs = pd.get_dummies(train_imgs)\n",
    "test_imgs = pd.get_dummies(test_imgs)\n",
    "#print(train_ds)\n",
    "\n",
    "train_dl = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)#TrainSet Labels\n",
    "test_dl = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)#TestSet Labels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "# 假设您的数据集已经准备好，包括训练集和验证集\n",
    "# 这里仅作示例，您需要根据实际情况修改数据加载部分\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in train_dl:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 初始化ResNet模型\n",
    "model = resnet18(pretrained=True)\n",
    "model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "num_classes = 2\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# 设置优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(30):  # 假设训练10个epoch\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dl:\n",
    "        images, labels = images.to(device), labels.to(device)  # 将图像和标签分别移动到设备上\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.to(torch.int64))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/30], Loss: {running_loss / len(train_dl)}\")\n",
    "\n",
    "\n",
    "\n",
    "summary(model, input_size=(4,256,256), batch_size=-1, device='cuda')\n",
    "# 验证模型\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dl:\n",
    "        images, labels = images.to(device), labels.to(device)  # 将图像和标签分别移动到设备上\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_list=[]\n",
    "False_list=[]\n",
    "for i in range(1,11):\n",
    "    True_list.append('AML_LAB/imu_data/True_%dxizhi.csv'%i)\n",
    "    False_list.append('AML_LAB/imu_data/False_%dxizhi.csv'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ori,vary in zip(True_list[:6],False_list[:6]):\n",
    "    df = pd.read_csv(ori)\n",
    "    # 提取所需的数据列\n",
    "    x = df['roll']\n",
    "    y = df['pitch']\n",
    "    z = df['yaw']\n",
    "\n",
    "    # 创建3D图形\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "    # 绘制散点图\n",
    "    axes[0]=fig.add_subplot(1,2,1,projection='3d')\n",
    "    axes[0].scatter(x, y, z, c='r', marker='o')\n",
    "    axes[0].set_title('True')\n",
    "    # 设置坐标轴标签\n",
    "    axes[0].set_xlabel('Roll')\n",
    "    axes[0].set_ylabel('Pitch')\n",
    "    axes[0].set_zlabel('Yaw')\n",
    "    df = pd.read_csv(vary)\n",
    "    # 提取所需的数据列\n",
    "    x = df['roll']\n",
    "    y = df['pitch']\n",
    "    z = df['yaw']\n",
    "    axes[0]=fig.add_subplot(1,2,1,projection='3d')\n",
    "    axes[0].scatter(x, y, z, c='b', marker='o')\n",
    "    axes[0].set_title('True')\n",
    "    # 设置坐标轴标签\n",
    "    axes[0].set_xlabel('Roll')\n",
    "    axes[0].set_ylabel('Pitch')\n",
    "    axes[0].set_zlabel('Yaw')\n",
    "    # axes[1]=fig.add_subplot(1,2,2,projection='3d')\n",
    "    # axes[1].scatter(x, y, z, c='b', marker='o')\n",
    "    # axes[1].set_title('False')\n",
    "    # # 设置坐标轴标签\n",
    "    # axes[1].set_xlabel('Roll')\n",
    "    # axes[1].set_ylabel('Pitch')\n",
    "    # axes[1].set_zlabel('Yaw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 假设True_list和False_list包含文件路径列表\n",
    "\n",
    "# 仅选择前6个文件进行示例\n",
    "for ori, vary in zip(True_list[:], False_list[:]):\n",
    "    # 创建一个新的图形\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # 读取True数据并绘制散点图（红色）\n",
    "    df_true = pd.read_csv(ori)\n",
    "    x_true = df_true['roll']\n",
    "    y_true = df_true['pitch']\n",
    "    z_true = df_true['yaw']\n",
    "    ax.scatter(x_true, y_true, z_true, c='r', marker='o', label='True')\n",
    "\n",
    "    # 读取False数据并绘制散点图（蓝色）\n",
    "    df_false = pd.read_csv(vary)\n",
    "    x_false = df_false['roll']\n",
    "    y_false = df_false['pitch']\n",
    "    z_false = df_false['yaw']\n",
    "    ax.scatter(x_false, y_false, z_false, c='b', marker='o', label='False')\n",
    "\n",
    "    # 设置坐标轴标签\n",
    "    ax.set_xlabel('Roll')\n",
    "    ax.set_ylabel('Pitch')\n",
    "    ax.set_zlabel('Yaw')\n",
    "\n",
    "    # 添加图例\n",
    "    ax.legend()\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    True_list.append('AML_LAB/imu_data/True_%dche.csv'%i)\n",
    "    False_list.append('AML_LAB/imu_data/False_%dche.csv'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6447244094488189\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设True_list和False_list包含文件路径列表\n",
    "\n",
    "# 创建一个空的DataFrame来存储所有数据\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# 加载True数据并添加标签\n",
    "for ori in True_list[:]:\n",
    "    df = pd.read_csv(ori)\n",
    "    df['label'] = 1  # True数据标签为1\n",
    "    data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "# 加载False数据并添加标签\n",
    "for vary in False_list[:]:\n",
    "    df = pd.read_csv(vary)\n",
    "    df['label'] = 0  # False数据标签为0\n",
    "    data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = data[['roll', 'pitch', 'yaw']]\n",
    "y = data['label']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)#, random_state=40)\n",
    "\n",
    "# 创建并训练随机森林分类器\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'AML_LAB'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Lycho556/AML_LAB.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
